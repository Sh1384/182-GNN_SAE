{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# GNN Benchmarking Analysis - Google Colab Version\n",
    "\n",
    "This notebook runs the same comprehensive benchmarking analysis as `benchmarking.py` but in Google Colab.\n",
    "\n",
    "**Features**:\n",
    "- Multi-seed training (GCN, GAT, MLP, MeanMedian)\n",
    "- Statistical analysis with confidence intervals\n",
    "- Pairwise comparisons (Wilcoxon test + rank-biserial effect sizes)\n",
    "- Motif-specific performance metrics\n",
    "- Sensitivity analysis (optional)\n",
    "- Complete visualizations\n",
    "\n",
    "**Setup time**: ~5-10 minutes\n",
    "**Runtime**: 30 min (5 seeds) to 3 hours (10 seeds + sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup: Mount Google Drive and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set working directory to your project folder\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/182-GNN_SAE')  # MODIFY THIS PATH\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torch-geometric pytorch-lightning\n",
    "!pip install -q pandas numpy scipy matplotlib seaborn networkx scikit-learn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"✓ All packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_cuda"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 2. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_all"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import from local benchmarking.py\n",
    "from gnn_train import (\n",
    "    GraphDataset, GCNModel, GATModel, GNNTrainer,\n",
    "    load_all_graphs, split_data, collate_fn, MOTIF_LABELS, MOTIF_TO_ID\n",
    ")\n",
    "from benchmarking import (\n",
    "    MeanMedianBaseline, MLPBaseline, DataVariationDataset, BaselineTrainer,\n",
    "    BenchmarkExperiment\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_config"
   },
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'n_seeds': 20,             # Number of random seeds (default: 20 for statistical rigor)\n",
    "    'num_epochs': 50,         # Maximum training epochs\n",
    "    'batch_size': 128,          # Batch size\n",
    "    'learning_rate': 1e-3,     # Learning rate\n",
    "    'run_sensitivity': True,  # Run sensitivity analysis? (adds ~1-2 hours)\n",
    "    'output_dir': 'outputs/benchmark_colab',  # Output directory\n",
    "}\n",
    "\n",
    "# Print configuration\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create output directory\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\n✓ Output directory created: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase1"
   },
   "source": [
    "## 4. Phase 1: Multi-Seed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "multi_seed_training"
   },
   "outputs": [],
   "source": [
    "# Initialize benchmark experiment\n",
    "benchmark = BenchmarkExperiment(output_dir=CONFIG['output_dir'])\n",
    "\n",
    "# Models to test\n",
    "models_to_test = ['GCN', 'GAT', 'MLP', 'MeanMedian']\n",
    "\n",
    "# Storage for results\n",
    "all_results = {}\n",
    "all_detailed_stats = {}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 1: MULTI-SEED TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train each model\n",
    "for model in models_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Run multi-seed training\n",
    "        results = benchmark.run_multi_seed_training(\n",
    "            model_type=model,\n",
    "            n_seeds=CONFIG['n_seeds'],\n",
    "            num_epochs=CONFIG['num_epochs'],\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            learning_rate=CONFIG['learning_rate']\n",
    "        )\n",
    "        \n",
    "        all_results[model] = results\n",
    "        \n",
    "        # Save individual results\n",
    "        benchmark.save_results(results, f\"{model.lower()}_results.json\")\n",
    "        \n",
    "        # Generate statistics\n",
    "        detailed_stats = benchmark.generate_detailed_statistics(results, model)\n",
    "        all_detailed_stats[model] = detailed_stats\n",
    "        \n",
    "        # Print results table\n",
    "        results_df = benchmark.generate_results_table(results, model)\n",
    "        print(f\"\\n{model} Results:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Multi-seed training complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "statistics"
   },
   "source": [
    "## 5. Phase 1b: Aggregate Statistics and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_statistics"
   },
   "outputs": [],
   "source": [
    "# Save detailed statistics\n",
    "if all_detailed_stats:\n",
    "    print(\"Saving comprehensive statistical summary...\")\n",
    "    \n",
    "    detailed_stats_dict = {\n",
    "        model: {k: v for k, v in stats.items() if k != 'test_loss' or isinstance(v, dict)}\n",
    "        for model, stats in all_detailed_stats.items()\n",
    "    }\n",
    "    benchmark.save_results(detailed_stats_dict, \"detailed_statistics.json\")\n",
    "    print(\"✓ Saved detailed_statistics.json\")\n",
    "\n",
    "# Save multi-seed summary\n",
    "benchmark.save_results(all_results, \"multi_seed_summary.json\")\n",
    "print(\"✓ Saved multi_seed_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "motif_metrics"
   },
   "source": [
    "## 6. Phase 1c: Motif-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compute_motif_metrics"
   },
   "outputs": [],
   "source": [
    "# Aggregate motif metrics across seeds\n",
    "print(\"Aggregating motif metrics across seeds...\")\n",
    "\n",
    "motif_metrics_all_models = {}\n",
    "for model, results in all_results.items():\n",
    "    if 'motif_metrics' in results and results['motif_metrics']:\n",
    "        # Average motif metrics across seeds\n",
    "        aggregated_motif_metrics = {}\n",
    "        for motif_label in results['motif_metrics'][0].keys():\n",
    "            mses = [seed_metrics[motif_label]['mean_mse'] \n",
    "                    for seed_metrics in results['motif_metrics'] \n",
    "                    if motif_label in seed_metrics]\n",
    "            maes = [seed_metrics[motif_label]['mean_mae'] \n",
    "                    for seed_metrics in results['motif_metrics'] \n",
    "                    if motif_label in seed_metrics]\n",
    "            num_graphs = results['motif_metrics'][0][motif_label]['num_graphs']\n",
    "            \n",
    "            aggregated_motif_metrics[motif_label] = {\n",
    "                'num_graphs': num_graphs,\n",
    "                'mean_mse': float(np.mean(mses)),\n",
    "                'std_mse': float(np.std(mses)) if len(mses) > 1 else 0.0,\n",
    "                'mean_mae': float(np.mean(maes)),\n",
    "                'std_mae': float(np.std(maes)) if len(maes) > 1 else 0.0\n",
    "            }\n",
    "        motif_metrics_all_models[model] = aggregated_motif_metrics\n",
    "\n",
    "# Save aggregated motif metrics\n",
    "if motif_metrics_all_models:\n",
    "    benchmark.save_results(motif_metrics_all_models, \"motif_metrics_summary.json\")\n",
    "    print(\"✓ Saved motif_metrics_summary.json\")\n",
    "else:\n",
    "    print(\"⚠ No motif metrics found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase2"
   },
   "source": [
    "## 7. Phase 2: Pairwise Statistical Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pairwise_comparisons"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: PAIRWISE STATISTICAL COMPARISONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute pairwise comparisons\n",
    "pairwise_comparisons = benchmark.compute_pairwise_comparisons(all_results)\n",
    "\n",
    "if pairwise_comparisons:\n",
    "    # Display results\n",
    "    print(\"\\nPairwise Comparison Results:\")\n",
    "    print()\n",
    "    \n",
    "    for comparison_label, results in pairwise_comparisons.items():\n",
    "        print(f\"\\n{comparison_label}:\")\n",
    "        print(f\"  Wilcoxon p-value: {results['p_value']:.6f}\")\n",
    "        print(f\"  Rank-biserial (r): {results['rank_biserial']:.4f}\")\n",
    "        print(f\"  95% CI: [{results['rank_biserial_ci_lower']:.4f}, {results['rank_biserial_ci_upper']:.4f}]\")\n",
    "        print(f\"  Mean Loss {comparison_label.split(' vs ')[0]}: {results['mean_loss_a']:.4f}\")\n",
    "        print(f\"  Mean Loss {comparison_label.split(' vs ')[1]}: {results['mean_loss_b']:.4f}\")\n",
    "        print(f\"  Better Model: {results['better_model']}\")\n",
    "        print(f\"  Significant (p<0.05): {results['is_significant']}\")\n",
    "    \n",
    "    # Save results\n",
    "    benchmark.save_results(pairwise_comparisons, \"pairwise_comparisons.json\")\n",
    "    print(\"\\n✓ Saved pairwise_comparisons.json\")\n",
    "else:\n",
    "    print(\"⚠ No pairwise comparisons computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualizations"
   },
   "source": [
    "## 8. Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gen_visualizations"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all_detailed_stats:\n",
    "    # Baseline comparison\n",
    "    try:\n",
    "        print(\"\\nGenerating baseline comparison plot...\")\n",
    "        benchmark.plot_baseline_comparison(all_detailed_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Seed variance\n",
    "    try:\n",
    "        print(\"Generating seed variance plot...\")\n",
    "        benchmark.plot_seed_variance(all_detailed_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Statistical summary table\n",
    "    try:\n",
    "        print(\"Generating statistical summary table...\")\n",
    "        benchmark.plot_statistical_summary_table(all_detailed_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Train/val/test progression\n",
    "    try:\n",
    "        print(\"Generating train/val/test progression plot...\")\n",
    "        benchmark.plot_train_val_test_progression(all_detailed_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Pairwise comparisons\n",
    "if pairwise_comparisons:\n",
    "    try:\n",
    "        print(\"Generating pairwise comparisons plot...\")\n",
    "        benchmark.plot_pairwise_comparisons(pairwise_comparisons)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n✓ Visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "motif_viz"
   },
   "source": [
    "## 9. Motif-Specific Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "motif_visualizations"
   },
   "outputs": [],
   "source": [
    "if motif_metrics_all_models:\n",
    "    print(\"\\nGenerating motif-specific visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Generating motif comparison plot...\")\n",
    "        benchmark.plot_motif_comparison(motif_metrics_all_models)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Generating motif heatmap...\")\n",
    "        benchmark.plot_motif_heatmap(motif_metrics_all_models)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"✓ Motif visualizations complete\")\n",
    "else:\n",
    "    print(\"No motif metrics to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sensitivity_optional"
   },
   "source": [
    "## 10. Sensitivity Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sensitivity_config"
   },
   "outputs": [],
   "source": [
    "# CHANGE THIS TO TRUE TO RUN SENSITIVITY ANALYSIS\n",
    "RUN_SENSITIVITY = CONFIG['run_sensitivity']\n",
    "\n",
    "if RUN_SENSITIVITY:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 4: SENSITIVITY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nThis will test GCN and GAT with different:\")\n",
    "    print(\"  - Timesteps: [25, 50, 75]\")\n",
    "    print(\"  - Noise levels: [0.005, 0.01, 0.05]\")\n",
    "    print(\"\\nEstimated time: 1-2 hours\")\n",
    "    \n",
    "    sensitivity_results = {}\n",
    "    \n",
    "    for model in ['GCN', 'GAT']:\n",
    "        try:\n",
    "            print(f\"\\nRunning sensitivity analysis for {model}...\")\n",
    "            results = benchmark.run_sensitivity_analysis(model_type=model)\n",
    "            sensitivity_results[model] = results\n",
    "            benchmark.save_results(results, f\"{model.lower()}_sensitivity.json\")\n",
    "            print(f\"✓ Saved {model.lower()}_sensitivity.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    # Generate sensitivity visualizations\n",
    "    if sensitivity_results:\n",
    "        try:\n",
    "            print(\"\\nGenerating combined sensitivity plot...\")\n",
    "            benchmark.plot_sensitivity_analysis(sensitivity_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Generating individual sensitivity plots...\")\n",
    "            benchmark.plot_individual_sensitivity_analysis(sensitivity_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        print(\"\\n✓ Sensitivity analysis complete\")\n",
    "else:\n",
    "    print(\"\\n⊘ Sensitivity analysis skipped (set RUN_SENSITIVITY=True to enable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 11. Summary and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "print_summary"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nAll results saved to: {CONFIG['output_dir']}\")\n",
    "\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"\\n  Visualizations:\")\n",
    "print(\"    - baseline_comparison.png\")\n",
    "print(\"    - seed_variance.png\")\n",
    "print(\"    - statistical_summary_table.png\")\n",
    "print(\"    - train_val_test_progression.png\")\n",
    "print(\"    - pairwise_comparisons.png\")\n",
    "\n",
    "if motif_metrics_all_models:\n",
    "    print(\"    - motif_comparison.png\")\n",
    "    print(\"    - motif_heatmap.png\")\n",
    "\n",
    "if RUN_SENSITIVITY:\n",
    "    print(\"    - sensitivity_analysis.png\")\n",
    "    print(\"    - gcn_sensitivity_detailed.png\")\n",
    "    print(\"    - gat_sensitivity_detailed.png\")\n",
    "\n",
    "print(\"\\n  Data Files:\")\n",
    "print(\"    - detailed_statistics.json\")\n",
    "print(\"    - gcn_results.json\")\n",
    "print(\"    - gat_results.json\")\n",
    "print(\"    - mlp_results.json\")\n",
    "print(\"    - meanmedian_results.json\")\n",
    "print(\"    - pairwise_comparisons.json\")\n",
    "print(\"    - motif_metrics_summary.json\")\n",
    "print(\"    - multi_seed_summary.json\")\n",
    "\n",
    "if RUN_SENSITIVITY:\n",
    "    print(\"    - gcn_sensitivity.json\")\n",
    "    print(\"    - gat_sensitivity.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 12. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_zip"
   },
   "outputs": [],
   "source": [
    "# Create zip file of all results\n",
    "import shutil\n",
    "\n",
    "output_path = Path(CONFIG['output_dir'])\n",
    "zip_name = \"benchmark_results\"\n",
    "\n",
    "print(f\"Creating zip file: {zip_name}.zip\")\n",
    "shutil.make_archive(zip_name, 'zip', output_path)\n",
    "\n",
    "print(f\"✓ Created {zip_name}.zip\")\n",
    "print(f\"\\nTo download:\")\n",
    "print(f\"  1. Click the folder icon on the left (Files)\")\n",
    "print(f\"  2. Find {zip_name}.zip\")\n",
    "print(f\"  3. Right-click and select Download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inspect_results"
   },
   "source": [
    "## 13. Inspect Key Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_statistics"
   },
   "outputs": [],
   "source": [
    "# Display detailed statistics as a nice table\n",
    "if all_detailed_stats:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED STATISTICS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for model_name, stats in all_detailed_stats.items():\n",
    "        test_loss = stats['test_loss']\n",
    "        summary_data.append({\n",
    "            'Model': model_name,\n",
    "            'Mean': f\"{test_loss['mean']:.4f}\",\n",
    "            'Median': f\"{test_loss['median']:.4f}\",\n",
    "            'Std': f\"{test_loss['std']:.4f}\",\n",
    "            '95% CI': f\"±{test_loss['ci_95']:.4f}\",\n",
    "            'Min': f\"{test_loss['min']:.4f}\",\n",
    "            'Max': f\"{test_loss['max']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_pairwise"
   },
   "outputs": [],
   "source": [
    "# Display pairwise comparison results\n",
    "if pairwise_comparisons:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PAIRWISE COMPARISON RESULTS (Wilcoxon Test)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    comparison_data = []\n",
    "    for comparison_label, results in pairwise_comparisons.items():\n",
    "        comparison_data.append({\n",
    "            'Comparison': comparison_label,\n",
    "            'p-value': f\"{results['p_value']:.6f}\",\n",
    "            'Rank-biserial (r)': f\"{results['rank_biserial']:.4f}\",\n",
    "            '95% CI': f\"[{results['rank_biserial_ci_lower']:.4f}, {results['rank_biserial_ci_upper']:.4f}]\",\n",
    "            'Significant': '✓ Yes' if results['is_significant'] else '✗ No',\n",
    "            'Better Model': results['better_model']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print()\n",
    "    print(\"Legend:\")\n",
    "    print(\"  p-value < 0.05: Statistically significant difference\")\n",
    "    print(\"  Rank-biserial |r| > 0.3: Medium to large effect size\")\n",
    "    print(\"  95% CI: Confidence interval - narrow = precise, wide = uncertain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plots"
   },
   "source": [
    "## 14. Display Generated Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_plots"
   },
   "outputs": [],
   "source": [
    "# Display baseline comparison plot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "\n",
    "plot_files = [\n",
    "    'baseline_comparison.png',\n",
    "    'seed_variance.png',\n",
    "    'statistical_summary_table.png',\n",
    "    'pairwise_comparisons.png',\n",
    "    'train_val_test_progression.png',\n",
    "]\n",
    "\n",
    "for plot_file in plot_files:\n",
    "    plot_path = output_dir / 'visualizations' / plot_file\n",
    "    if plot_path.exists():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{plot_file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        display(Image(str(plot_path)))\n",
    "    else:\n",
    "        print(f\"Plot not found: {plot_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_motif_plots"
   },
   "outputs": [],
   "source": [
    "# Display motif plots if available\n",
    "if motif_metrics_all_models:\n",
    "    motif_plots = ['motif_comparison.png', 'motif_heatmap.png']\n",
    "    \n",
    "    for plot_file in motif_plots:\n",
    "        plot_path = output_dir / 'visualizations' / plot_file\n",
    "        if plot_path.exists():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"{plot_file}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            display(Image(str(plot_path)))\n",
    "        else:\n",
    "            print(f\"Plot not found: {plot_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_sensitivity_plots"
   },
   "outputs": [],
   "source": [
    "# Display sensitivity plots if available\n",
    "if RUN_SENSITIVITY:\n",
    "    sensitivity_plots = [\n",
    "        'sensitivity_analysis.png',\n",
    "        'gcn_sensitivity_detailed.png',\n",
    "        'gat_sensitivity_detailed.png'\n",
    "    ]\n",
    "    \n",
    "    for plot_file in sensitivity_plots:\n",
    "        plot_path = output_dir / 'visualizations' / plot_file\n",
    "        if plot_path.exists():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"{plot_file}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            display(Image(str(plot_path)))\n",
    "        else:\n",
    "            print(f\"Plot not found: {plot_file}\")\n",
    "else:\n",
    "    print(\"Sensitivity analysis not run. Set RUN_SENSITIVITY=True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## Notes & Tips\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "To modify the analysis, edit the CONFIG dictionary in cell 3:\n",
    "\n",
    "```python\n",
    "CONFIG = {\n",
    "    'n_seeds': 5,              # More seeds = more rigorous (5-10 recommended)\n",
    "    'num_epochs': 100,         # Higher = longer training but potentially better\n",
    "    'batch_size': 32,          # Reduce if out of memory\n",
    "    'run_sensitivity': False,  # Set to True to run sensitivity analysis (adds 1-2 hours)\n",
    "    'output_dir': 'outputs/benchmark_colab',\n",
    "}\n",
    "```\n",
    "\n",
    "### Recommended Configurations\n",
    "\n",
    "**Quick test (15-30 min)**:\n",
    "```python\n",
    "n_seeds: 3\n",
    "num_epochs: 50\n",
    "run_sensitivity: False\n",
    "```\n",
    "\n",
    "**Standard evaluation (45 min - 1 hour)**:\n",
    "```python\n",
    "n_seeds: 5\n",
    "num_epochs: 100\n",
    "run_sensitivity: False\n",
    "```\n",
    "\n",
    "**Full analysis (2-3 hours)**:\n",
    "```python\n",
    "n_seeds: 5\n",
    "num_epochs: 100\n",
    "run_sensitivity: True\n",
    "```\n",
    "\n",
    "**Publication quality (3-4 hours)**:\n",
    "```python\n",
    "n_seeds: 10\n",
    "num_epochs: 150\n",
    "run_sensitivity: True\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Out of Memory Error**:\n",
    "- Reduce `batch_size` to 16\n",
    "- Reduce `n_seeds` to 3\n",
    "- Reduce `num_epochs` to 50\n",
    "\n",
    "**Timeout (Colab disconnects)**:\n",
    "- Colab has 12-hour time limit\n",
    "- Reduce number of seeds or epochs\n",
    "- Don't run sensitivity analysis with many seeds\n",
    "\n",
    "**Graphs not showing**:\n",
    "- Wait for training to complete\n",
    "- Check that all_detailed_stats is not empty\n",
    "- Rerun the visualization cells\n",
    "\n",
    "### Output Files\n",
    "\n",
    "All results are saved to the `outputs/benchmark_colab/` folder:\n",
    "- **Visualizations/**: PNG files of all plots\n",
    "- **Statistics/**: JSON files with raw data\n",
    "\n",
    "Download the `benchmark_results.zip` file to get everything at once.\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "See the `BENCHMARKING_WORKFLOW.md` and `SENSITIVITY_ANALYSIS_GUIDE.md` files for detailed interpretation guides.\n",
    "\n",
    "Key questions to ask:\n",
    "1. **Do GNNs outperform baselines?** (Check baseline_comparison.png and pairwise_comparisons.png)\n",
    "2. **Are results stable across seeds?** (Check seed_variance.png - tight box = stable)\n",
    "3. **Which model is best?** (Check statistical_summary_table.png - lowest mean test loss)\n",
    "4. **Are results significant?** (Check pairwise_comparisons.png - p < 0.05?)\n",
    "5. **Which motif types are hardest?** (Check motif_heatmap.png - darker = harder)\n",
    "6. **Is model robust to noise?** (Check sensitivity_analysis.png - flat = robust)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
