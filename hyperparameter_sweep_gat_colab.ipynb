{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT Hyperparameter Sweep with Optuna + Motif Metrics\n",
    "\n",
    "This notebook performs systematic hyperparameter optimization for the GAT (Graph Attention Network) model using Optuna on Google Colab, then trains the best model and computes motif-specific metrics.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Mount Google Drive** (to access your project files)\n",
    "2. **Install Dependencies** (Optuna if not already installed)\n",
    "3. **Run the Sweep** (50 trials by default, ~3-5 hours)\n",
    "4. **Train Best Model and Compute Motif Metrics** (~30 minutes)\n",
    "5. **View Results** (visualizations and metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive and Set Up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set working directory to your project\n",
    "project_dir = '/content/drive/My Drive/182-GNN_SAE'  # Adjust path if needed\n",
    "os.chdir(project_dir)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"\\nDirectory contents:\")\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install optuna -q\n",
    "!pip install torch-geometric -q\n",
    "\n",
    "print(\"✓ Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nfrom pathlib import Path\nfrom typing import List, Tuple\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom optuna.trial import Trial\nfrom optuna.samplers import TPESampler\n# from optuna.pruners import MedianPruner\nimport torch\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Check GPU availability\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Import Functions from gnn_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from gnn_train.py\n",
    "from gnn_train import (\n",
    "    GraphDataset,\n",
    "    GATModel,\n",
    "    GNNTrainer,\n",
    "    collate_fn,\n",
    "    load_all_graphs,\n",
    "    split_data,\n",
    "    MOTIF_LABELS,\n",
    "    MOTIF_TO_ID\n",
    ")\n",
    "\n",
    "print(\"✓ Successfully imported all functions from gnn_train.py\")\n",
    "\n",
    "# Helper function to compute graph-level motif metrics (from benchmarking.py approach)\n",
    "def compute_motif_metrics_graphlevel(model, test_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compute MSE and MAE metrics per motif type using graph-level aggregation.\n",
    "    This ensures each graph contributes equally to the final statistics.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model to evaluate\n",
    "        test_loader: DataLoader with test data (can have mixed motifs)\n",
    "        device: Device to run on\n",
    "\n",
    "    Returns:\n",
    "        Dict with per-motif metrics: {motif_label: {mean_mse, std_mse, mean_mae, std_mae, num_graphs}}\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    criterion_mse = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "    motif_losses = defaultdict(list)\n",
    "    motif_counts = defaultdict(int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "\n",
    "            # Compute per-node losses\n",
    "            loss_per_node_mse = criterion_mse(pred, batch.y)\n",
    "\n",
    "            # Extract motif information from batch\n",
    "            if hasattr(batch, 'batch'):\n",
    "                batch_indices = batch.batch\n",
    "            else:\n",
    "                batch_indices = torch.zeros(batch.y.size(0), dtype=torch.long, device=device)\n",
    "\n",
    "            unique_graphs = torch.unique(batch_indices)\n",
    "\n",
    "            for g in unique_graphs:\n",
    "                node_mask = batch_indices == g\n",
    "                masked_nodes = batch.mask[node_mask]\n",
    "                node_losses_mse = loss_per_node_mse[node_mask]\n",
    "                masked_losses_mse = node_losses_mse[masked_nodes]\n",
    "\n",
    "                if masked_losses_mse.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                # Compute MSE\n",
    "                graph_mse = masked_losses_mse.mean().item()\n",
    "\n",
    "                # Compute MAE\n",
    "                masked_preds = pred[node_mask][masked_nodes]\n",
    "                masked_targets = batch.y[node_mask][masked_nodes]\n",
    "                graph_mae = torch.abs(masked_preds - masked_targets).mean().item()\n",
    "\n",
    "                # Get motif label from batch\n",
    "                motif_label = \"unknown\"\n",
    "                if hasattr(batch, 'motif_id'):\n",
    "                    try:\n",
    "                        motif_tensor = batch.motif_id[g]\n",
    "                        motif_idx = int(motif_tensor.item())\n",
    "                        if 0 <= motif_idx < len(MOTIF_LABELS):\n",
    "                            motif_label = MOTIF_LABELS[motif_idx]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Store metrics\n",
    "                motif_losses[motif_label].append({\n",
    "                    'mse': graph_mse,\n",
    "                    'mae': graph_mae\n",
    "                })\n",
    "                motif_counts[motif_label] += 1\n",
    "\n",
    "    # Aggregate metrics per motif\n",
    "    metrics = {}\n",
    "    for motif_label in sorted(motif_losses.keys()):\n",
    "        losses = motif_losses[motif_label]\n",
    "        mses = [l['mse'] for l in losses]\n",
    "        maes = [l['mae'] for l in losses]\n",
    "\n",
    "        metrics[motif_label] = {\n",
    "            'num_graphs': motif_counts[motif_label],\n",
    "            'mean_mse': float(np.mean(mses)),\n",
    "            'std_mse': float(np.std(mses)) if len(mses) > 1 else 0.0,\n",
    "            'mean_mae': float(np.mean(maes)),\n",
    "            'std_mae': float(np.std(maes)) if len(maes) > 1 else 0.0\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"✓ Graph-level motif metrics helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Objective Function for GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def objective(trial: Trial, train_loader: DataLoader, val_loader: DataLoader,\n              test_loader: DataLoader, device: str, num_epochs: int = 100) -> float:\n    \"\"\"\n    Optuna objective function to minimize validation loss for GAT model.\n    Uses fixed batch size of 128 for fair comparison across trials.\n    \"\"\"\n    # Suggest hyperparameters specific to GAT (excluding batch_size for consistency)\n    hidden_dim = trial.suggest_int('hidden_dim', 16, 128, step=8)\n    num_heads = trial.suggest_int('num_heads', 2, 8, step=2)\n    dropout = trial.suggest_float('dropout', 0.0, 0.5, step=0.05)\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n\n    # Create GAT model with suggested hyperparameters\n    model = GATModel(\n        input_dim=2,\n        hidden_dim=hidden_dim,\n        output_dim=1,\n        dropout=dropout,\n        num_heads=num_heads,\n        edge_dim=1\n    )\n    model = model.to(device)\n    trainer = GNNTrainer(model, device=device, learning_rate=learning_rate)\n\n    # Training loop (using original loaders with fixed batch_size=128)\n    best_val_loss = float('inf')\n    patience_counter = 0\n    early_stopping_patience = 25\n\n    for epoch in range(num_epochs):\n        train_loss = trainer.train_epoch(train_loader)\n        val_loss = trainer.validate(val_loader)\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= early_stopping_patience:\n                break\n\n        # trial.report(val_loss, epoch)\n        # if trial.should_prune():\n        #     raise optuna.TrialPruned()\n\n    return best_val_loss\n\nprint(\"✓ Objective function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_distribution(trials_df: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"Plot distribution of validation losses.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].hist(trials_df['value'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(trials_df['value'].min(), color='red', linestyle='--', linewidth=2,\n",
    "                    label=f'Best: {trials_df[\"value\"].min():.4f}')\n",
    "    axes[0].axvline(trials_df['value'].mean(), color='green', linestyle='--', linewidth=2,\n",
    "                    label=f'Mean: {trials_df[\"value\"].mean():.4f}')\n",
    "    axes[0].set_xlabel('Validation Loss', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('Loss Distribution', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    box_data = trials_df['value'].values\n",
    "    bp = axes[1].boxplot(box_data, vert=True, patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    axes[1].set_ylabel('Validation Loss', fontsize=12)\n",
    "    axes[1].set_title('Loss Box Plot', fontsize=13, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1].set_xticklabels(['All Trials'])\n",
    "\n",
    "    stats_text = f\"\"\"Min: {trials_df['value'].min():.6f}\n",
    "Max: {trials_df['value'].max():.6f}\n",
    "Mean: {trials_df['value'].mean():.6f}\n",
    "Std: {trials_df['value'].std():.6f}\"\"\"\n",
    "    axes[1].text(1.3, trials_df['value'].mean(), stats_text, fontsize=10,\n",
    "                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = output_dir / 'loss_distribution.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  Saved: {output_path.name}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_hyperparameter_heatmap(trials_df: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"Plot heatmap of hyperparameters for top trials.\"\"\"\n",
    "    top_n = 10\n",
    "    param_cols = [col for col in trials_df.columns if col.startswith('params_')]\n",
    "\n",
    "    if not param_cols:\n",
    "        return\n",
    "\n",
    "    param_names = [col.replace('params_', '') for col in param_cols]\n",
    "\n",
    "    top_trials = trials_df.nsmallest(top_n, 'value')[param_cols].copy()\n",
    "    top_trials.columns = param_names\n",
    "\n",
    "    for col in top_trials.columns:\n",
    "        min_val = trials_df[f'params_{col}'].min()\n",
    "        max_val = trials_df[f'params_{col}'].max()\n",
    "        if max_val > min_val:\n",
    "            top_trials[col] = (top_trials[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.heatmap(\n",
    "        top_trials.T,\n",
    "        annot=False,\n",
    "        cmap='RdYlGn',\n",
    "        cbar_kws={'label': 'Normalized Value'},\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'Top {top_n} Trials Hyperparameters (Normalized)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Trial Rank (Best → Worst)', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = output_dir / 'top_trials_heatmap.png'\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"  Saved: {output_path.name}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_optuna_visualizations(study, output_dir: Path):\n",
    "    \"\"\"Generate Optuna's built-in visualization functions.\"\"\"\n",
    "    try:\n",
    "        # Optimization history\n",
    "        fig = optuna.visualization.plot_optimization_history(study)\n",
    "        fig.write_html(str(output_dir / 'optuna_optimization_history.html'))\n",
    "        print(f\"  Saved: optuna_optimization_history.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not generate optimization history: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Parameter importances\n",
    "        fig = optuna.visualization.plot_param_importances(study)\n",
    "        fig.write_html(str(output_dir / 'optuna_param_importances.html'))\n",
    "        print(f\"  Saved: optuna_param_importances.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not generate param importances: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Slice plot (parameter distributions)\n",
    "        fig = optuna.visualization.plot_slice(study)\n",
    "        fig.write_html(str(output_dir / 'optuna_slice_plot.html'))\n",
    "        print(f\"  Saved: optuna_slice_plot.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not generate slice plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Parallel coordinates (parameter interactions)\n",
    "        fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "        fig.write_html(str(output_dir / 'optuna_parallel_coordinates.html'))\n",
    "        print(f\"  Saved: optuna_parallel_coordinates.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not generate parallel coordinates: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Contour plot (parameter interactions)\n",
    "        fig = optuna.visualization.plot_contour(study)\n",
    "        fig.write_html(str(output_dir / 'optuna_contour_plot.html'))\n",
    "        print(f\"  Saved: optuna_contour_plot.html\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Could not generate contour plot: {e}\")\n",
    "\n",
    "\n",
    "def generate_visualizations(trials_df: pd.DataFrame, study, output_dir: Path):\n",
    "    \"\"\"Generate all visualizations from trials.\"\"\"\n",
    "    print(\"\\nGenerating custom visualizations...\")\n",
    "    plot_loss_distribution(trials_df, output_dir)\n",
    "    plot_hyperparameter_heatmap(trials_df, output_dir)\n",
    "\n",
    "    print(\"\\nGenerating Optuna visualizations...\")\n",
    "    generate_optuna_visualizations(study, output_dir)\n",
    "\n",
    "print(\"✓ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "CONFIG = {\n",
    "    'data_dir': 'virtual_graphs/data',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_trials': 20,\n",
    "    'num_epochs': 100,\n",
    "    'output_dir': f'outputs/hyperparameter_sweep_gat_{seed}',\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(CONFIG['seed'])\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "Path(CONFIG['output_dir']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "graph_paths = load_all_graphs(CONFIG['data_dir'], single_motif_only=False)\n",
    "print(f\"Loaded {len(graph_paths)} graphs\")\n",
    "\n",
    "train_paths, val_paths, test_paths = split_data(graph_paths, train_ratio=0.8, val_ratio=0.1, stratify_by_motif=True)\n",
    "print(f\"Train: {len(train_paths)}, Val: {len(val_paths)}, Test: {len(test_paths)}\")\n",
    "\n",
    "train_dataset = GraphDataset(train_paths, mask_prob=0.2, seed=CONFIG['seed'])\n",
    "val_dataset = GraphDataset(val_paths, mask_prob=0.2, seed=CONFIG['seed'])\n",
    "test_dataset = GraphDataset(test_paths, mask_prob=0.2, seed=CONFIG['seed'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"✓ Data loading complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run Hyperparameter Sweep\n",
    "\n",
    "**⏱️ This may take 3-5 hours for 50 trials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"GAT HYPERPARAMETER SWEEP WITH OPTUNA\")\nprint(\"=\" * 80)\nprint(f\"Running {CONFIG['num_trials']} trials with up to {CONFIG['num_epochs']} epochs each\\n\")\n\nsampler = TPESampler(seed=CONFIG['seed'])\n# pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=10)\n\nstudy = optuna.create_study(\n    direction='minimize',\n    sampler=sampler,\n    # pruner=pruner,\n    study_name='gat_optimization'\n)\n\nstudy.optimize(\n    lambda trial: objective(trial, train_loader, val_loader, test_loader, CONFIG['device'], CONFIG['num_epochs']),\n    n_trials=CONFIG['num_trials'],\n    show_progress_bar=True\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Display and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"\\nBest Trial: {best_trial.number}\")\n",
    "print(f\"Best Validation Loss: {best_trial.value:.6f}\")\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save results\n",
    "output_path = Path(CONFIG['output_dir'])\n",
    "print(f\"\\nSaving results to {CONFIG['output_dir']}...\")\n",
    "\n",
    "with open(output_path / \"best_params.json\", 'w') as f:\n",
    "    json.dump(best_trial.params, f, indent=2)\n",
    "\n",
    "with open(output_path / \"study_info.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'best_value': best_trial.value,\n",
    "        'best_trial': best_trial.number,\n",
    "        'n_trials': len(study.trials),\n",
    "        'n_complete_trials': len([t for t in study.trials if t.state.name == 'COMPLETE'])\n",
    "    }, f, indent=2)\n",
    "\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df.to_csv(output_path / \"trials.csv\", index=False)\n",
    "print(f\"✓ Saved results to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Generate Sweep Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_visualizations(trials_df, study, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Train Top 5 Models and Compute Motif Metrics\n",
    "\n",
    "**⏱️ This takes ~2-3 hours for 5 models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING TOP 5 TRIALS AND EXTRACTING BEST MODEL ARTIFACTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get top 5 trials (best trial is the first one)\n",
    "sorted_trials = sorted(study.trials, key=lambda t: t.value)[:5]\n",
    "best_trial = sorted_trials[0]\n",
    "\n",
    "print(f\"\\nBest Trial: Trial {best_trial.number} with Val Loss = {best_trial.value:.6f}\")\n",
    "print(f\"\\nTop 5 Trial Rankings:\")\n",
    "for idx, trial in enumerate(sorted_trials, 1):\n",
    "    print(f\"  {idx}. Trial {trial.number}: Val Loss = {trial.value:.6f}\")\n",
    "\n",
    "# Dictionary to store metrics for top 5 trials\n",
    "top5_motif_metrics = {}\n",
    "\n",
    "for trial_idx, trial in enumerate(sorted_trials):\n",
    "    trial_name = f\"Best_Trial_{best_trial.number}\" if trial == best_trial else f\"Trial_{trial.number}\"\n",
    "    print(f\"\\n--- Processing {trial_name} ---\")\n",
    "    \n",
    "    # Extract hyperparameters for this trial\n",
    "    trial_hidden_dim = int(trial.params['hidden_dim'])\n",
    "    trial_num_heads = int(trial.params['num_heads'])\n",
    "    trial_dropout = float(trial.params['dropout'])\n",
    "    trial_learning_rate = float(trial.params['learning_rate'])\n",
    "    \n",
    "    # Reset seeds for deterministic batch sampling (fixed batch_size=128)\n",
    "    torch.manual_seed(CONFIG['seed'])\n",
    "    np.random.seed(CONFIG['seed'])\n",
    "    \n",
    "    # Create dataloaders with fixed batch_size=128 (deterministic due to seed reset)\n",
    "    trial_train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "    trial_val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "    trial_test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Create and train GAT model with this trial's hyperparameters\n",
    "    trial_model = GATModel(\n",
    "        input_dim=2,\n",
    "        hidden_dim=trial_hidden_dim,\n",
    "        output_dim=1,\n",
    "        dropout=trial_dropout,\n",
    "        num_heads=trial_num_heads,\n",
    "        edge_dim=1\n",
    "    )\n",
    "    trial_model = trial_model.to(CONFIG['device'])\n",
    "    trial_trainer = GNNTrainer(trial_model, device=CONFIG['device'], learning_rate=trial_learning_rate)\n",
    "    \n",
    "    # Training loop with early stopping\n",
    "    trial_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stopping_patience = 25\n",
    "    \n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        train_loss = trial_trainer.train_epoch(trial_train_loader)\n",
    "        val_loss = trial_trainer.validate(trial_val_loader)\n",
    "        \n",
    "        if val_loss < trial_val_loss:\n",
    "            trial_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                break\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss = trial_trainer.validate(trial_test_loader)\n",
    "    print(f\"  Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "    # Save model and activations only for the best (top-ranked) trial\n",
    "    if trial == best_trial:\n",
    "        print(f\"  Saving best model and activations...\")\n",
    "        model_save_path = output_path / \"best_model.pt\"\n",
    "        trial_trainer.save_model(str(model_save_path))\n",
    "        print(f\"  ✓ Saved best model to {model_save_path}\")\n",
    "    \n",
    "    # Compute motif metrics for this trial using graph-level aggregation\n",
    "    print(f\"  Computing motif metrics (graph-level aggregation)...\")\n",
    "    train_motif_metrics = compute_motif_metrics_graphlevel(trial_trainer.model, trial_train_loader, CONFIG['device'])\n",
    "    val_motif_metrics = compute_motif_metrics_graphlevel(trial_trainer.model, trial_val_loader, CONFIG['device'])\n",
    "    test_motif_metrics = compute_motif_metrics_graphlevel(trial_trainer.model, trial_test_loader, CONFIG['device'])\n",
    "    \n",
    "    top5_motif_metrics[f\"Trial_{trial.number}\"] = {\n",
    "        'train': train_motif_metrics,\n",
    "        'val': val_motif_metrics,\n",
    "        'test': test_motif_metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ {trial_name} complete\")\n",
    "\n",
    "print(\"\\n✓ All top 5 trials processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Display Motif Metrics for Top 5 Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MOTIF-SPECIFIC METRICS (Top 5 Trials)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for trial_name, trial_data in top5_motif_metrics.items():\n",
    "    print(f\"\\n\\n{'=' * 80}\")\n",
    "    print(f\"{trial_name.upper()}\")\n",
    "    print('=' * 80)\n",
    "    \n",
    "    for split_name, metrics in trial_data.items():\n",
    "        print(f\"\\n{split_name.upper()} SET:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        if not metrics:\n",
    "            print(\"  No metrics computed\")\n",
    "            continue\n",
    "        \n",
    "        for motif_label in sorted(metrics.keys()):\n",
    "            motif_data = metrics[motif_label]\n",
    "            print(f\"\\n  {motif_label.upper()}:\")\n",
    "            print(f\"    Num Graphs:    {motif_data.get('num_graphs', 'N/A')}\")\n",
    "            print(f\"    Mean MSE:      {motif_data['mean_mse']:.6f}\")\n",
    "            print(f\"    Std MSE:       {motif_data['std_mse']:.6f}\")\n",
    "            print(f\"    Mean MAE:      {motif_data['mean_mae']:.6f}\")\n",
    "            print(f\"    Std MAE:       {motif_data['std_mae']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Save and Visualize Motif Metrics for Top 5 Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save motif metrics for top 5 trials\n",
    "motif_metrics_path = output_path / \"motif_metrics_top5_trials.json\"\n",
    "with open(motif_metrics_path, 'w') as f:\n",
    "    json.dump(top5_motif_metrics, f, indent=2)\n",
    "print(f\"✓ Saved top 5 trials motif metrics to {motif_metrics_path}\")\n",
    "\n",
    "# Create visualizations by motif type (averaged over top 5 trials)\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "for split_name in splits:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Get motif types from first trial\n",
    "    first_trial_key = f\"Trial_{sorted_trials[0].number}\"\n",
    "    motif_types = list(sorted(list(top5_motif_metrics[first_trial_key][split_name].keys())))\n",
    "    \n",
    "    # MSE by Motif Type\n",
    "    ax = axes[0]\n",
    "    x_pos = np.arange(len(motif_types))\n",
    "    mse_means = []\n",
    "    mse_stds = []\n",
    "    \n",
    "    for motif in motif_types:\n",
    "        all_mses = []\n",
    "        for trial in sorted_trials:\n",
    "            trial_key = f\"Trial_{trial.number}\"\n",
    "            if motif in top5_motif_metrics[trial_key][split_name]:\n",
    "                all_mses.append(top5_motif_metrics[trial_key][split_name][motif]['mean_mse'])\n",
    "        if all_mses:\n",
    "            mse_means.append(np.mean(all_mses))\n",
    "            mse_stds.append(np.std(all_mses))\n",
    "        else:\n",
    "            mse_means.append(0)\n",
    "            mse_stds.append(0)\n",
    "    \n",
    "    ax.bar(x_pos, mse_means, yerr=mse_stds, capsize=5, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "    ax.set_xlabel('Motif Type', fontsize=12)\n",
    "    ax.set_ylabel('MSE', fontsize=12)\n",
    "    ax.set_title(f'MSE by Motif Type ({split_name.upper()}, Averaged over Top 5 Trials)', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(motif_types, rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # MAE by Motif Type\n",
    "    ax = axes[1]\n",
    "    mae_means = []\n",
    "    mae_stds = []\n",
    "    \n",
    "    for motif in motif_types:\n",
    "        all_maes = []\n",
    "        for trial in sorted_trials:\n",
    "            trial_key = f\"Trial_{trial.number}\"\n",
    "            if motif in top5_motif_metrics[trial_key][split_name]:\n",
    "                motif_data = top5_motif_metrics[trial_key][split_name][motif]\n",
    "                # Calculate MAE from mean and std MSE if available\n",
    "                if 'mean_mse' in motif_data:\n",
    "                    all_maes.append(np.sqrt(motif_data['mean_mse']))\n",
    "        if all_maes:\n",
    "            mae_means.append(np.mean(all_maes))\n",
    "            mae_stds.append(np.std(all_maes))\n",
    "        else:\n",
    "            mae_means.append(0)\n",
    "            mae_stds.append(0)\n",
    "    \n",
    "    ax.bar(x_pos, mae_means, yerr=mae_stds, capsize=5, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "    ax.set_xlabel('Motif Type', fontsize=12)\n",
    "    ax.set_ylabel('MAE', fontsize=12)\n",
    "    ax.set_title(f'MAE by Motif Type ({split_name.upper()}, Averaged over Top 5 Trials)', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(motif_types, rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    motif_viz_path = output_path / f'motif_comparison_{split_name}.png'\n",
    "    plt.savefig(motif_viz_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved {split_name} motif comparison to {motif_viz_path}\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST MODEL MOTIF-SPECIFIC METRICS VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create detailed visualizations for the best model only\n",
    "best_trial_key = f\"Trial_{best_trial.number}\"\n",
    "best_metrics = top5_motif_metrics[best_trial_key]\n",
    "\n",
    "for split_name in splits:\n",
    "    # Get motif types and their metrics for best model\n",
    "    split_metrics = best_metrics[split_name]\n",
    "    motif_types_best = sorted(split_metrics.keys())\n",
    "    \n",
    "    # Extract MSE and MAE values for best model\n",
    "    mse_values = [split_metrics[motif]['mean_mse'] for motif in motif_types_best]\n",
    "    mse_stds = [split_metrics[motif]['std_mse'] for motif in motif_types_best]\n",
    "    mae_values = [np.sqrt(split_metrics[motif]['mean_mse']) for motif in motif_types_best]\n",
    "    mae_stds = [np.sqrt(split_metrics[motif]['std_mse']) for motif in motif_types_best]\n",
    "    \n",
    "    # Create comprehensive figure for best model\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. MSE comparison bar chart\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    x_pos = np.arange(len(motif_types_best))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(motif_types_best)))\n",
    "    bars1 = ax1.bar(x_pos, mse_values, yerr=mse_stds, capsize=8, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax1.set_xlabel('Motif Type', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('MSE', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(f'MSE by Motif Type - Best Model ({split_name.upper()})', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(motif_types_best, rotation=45, ha='right', fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars1, mse_values)):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, val + mse_stds[i], f'{val:.4f}', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 2. MAE comparison bar chart\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    bars2 = ax2.bar(x_pos, mae_values, yerr=mae_stds, capsize=8, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    ax2.set_xlabel('Motif Type', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('MAE', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title(f'MAE by Motif Type - Best Model ({split_name.upper()})', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(motif_types_best, rotation=45, ha='right', fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars2, mae_values)):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, val + mae_stds[i], f'{val:.4f}', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 3. MSE and MAE side-by-side comparison\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    width = 0.35\n",
    "    x_pos_grouped = np.arange(len(motif_types_best))\n",
    "    bars3a = ax3.bar(x_pos_grouped - width/2, mse_values, width, label='MSE', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "    bars3b = ax3.bar(x_pos_grouped + width/2, mae_values, width, label='MAE', color='coral', alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    ax3.set_xlabel('Motif Type', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Error Value', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title(f'MSE vs MAE Comparison - Best Model ({split_name.upper()})', fontsize=13, fontweight='bold')\n",
    "    ax3.set_xticks(x_pos_grouped)\n",
    "    ax3.set_xticklabels(motif_types_best, rotation=45, ha='right', fontsize=11)\n",
    "    ax3.legend(fontsize=11, loc='upper left')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Error statistics table\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.axis('tight')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    table_data = []\n",
    "    table_data.append(['Motif Type', 'MSE', 'Std MSE', 'MAE', 'Std MAE'])\n",
    "    for i, motif in enumerate(motif_types_best):\n",
    "        table_data.append([\n",
    "            motif,\n",
    "            f'{mse_values[i]:.6f}',\n",
    "            f'{mse_stds[i]:.6f}',\n",
    "            f'{mae_values[i]:.6f}',\n",
    "            f'{mae_stds[i]:.6f}'\n",
    "        ])\n",
    "    \n",
    "    table = ax4.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                     colWidths=[0.25, 0.18, 0.18, 0.18, 0.18])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style header row\n",
    "    for i in range(len(table_data[0])):\n",
    "        table[(0, i)].set_facecolor('#40466e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Alternate row colors\n",
    "    for i in range(1, len(table_data)):\n",
    "        for j in range(len(table_data[0])):\n",
    "            if i % 2 == 0:\n",
    "                table[(i, j)].set_facecolor('#f0f0f0')\n",
    "            else:\n",
    "                table[(i, j)].set_facecolor('#ffffff')\n",
    "    \n",
    "    fig.suptitle(f'Best Model (Trial {best_trial.number}) - Motif-Specific Metrics\\n{split_name.upper()} Split', \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.savefig(output_path / f'best_model_motif_metrics_{split_name}.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved best model motif metrics visualization for {split_name}\")\n",
    "    plt.close()\n",
    "\n",
    "# Save summary of best model (which is the top-ranked trial)\n",
    "best_summary_path = output_path / \"best_model_summary.json\"\n",
    "with open(best_summary_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'best_trial_number': best_trial.number,\n",
    "        'best_validation_loss': float(best_trial.value),\n",
    "        'hyperparameters': best_trial.params,\n",
    "        'motif_metrics': top5_motif_metrics[best_trial_key]\n",
    "    }, f, indent=2)\n",
    "print(f\"✓ Saved best model summary to {best_summary_path}\")\n",
    "\n",
    "print(\"\\n✓ All motif-specific metrics visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nResults location: {output_path}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "for file in sorted(output_path.iterdir()):\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL TASKS COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}