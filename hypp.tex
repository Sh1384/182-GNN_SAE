\subsection{Hyperparameter choices}
\label{sec:hyperparams}

We report the hyperparameters used for all experiments and provide concise motivations tied to prior literature. Where possible we adopt values recommended by canonical GNN and masked-autoencoder studies to ensure reproducibility and fair model comparison.

\paragraph{Batch size.} We train all models with \texttt{batch\_size = 128}. This is consistent with prior large-scale GNN representation studies that report stable optimization dynamics in the range $\{64,128\}$ for mini-batch training. In particular, the MISATO framework for probing mechanistic interpretability in GNNs \citep{Misato2024} performs all SAE-based circuit extraction experiments using batch sizes between 64 and 128 for both pretraining and probing phases, noting that (i) these batch sizes yield low-variance gradient estimates that preserve feature-level structure in the learned representations, and (ii) larger batches reduce stochasticity that otherwise obscures motif-level attribution. MISATO further reports that increasing the batch size beyond 128 provides no measurable improvement in reconstruction or interpretability metrics, whereas smaller batches increase variance in SAE feature discovery. Following this guidance, we adopt 128 as a stable, memory-efficient setting that ensures comparability with recent interpretability work while maintaining efficient GPU utilization.

\paragraph{Model widths and second-layer size.} To make activations comparable between architectures we use a second-layer output dimension of 64 for all models (GCN, GAT) and for the SAE analysis. For GAT this matches the original architecture choice (8 heads with 8 features each $\rightarrow 8\times8=64$) which is the setting reported by Veli\v{c}kovi\'c \etal. \citep{Velickovic2018}. The original GCN paper used smaller hidden sizes for the small citation benchmarks (e.g., 16) \citep{Kipf2017}; we intentionally increase the GCN output to 64 so that downstream SAE analyses receive embeddings of equal dimension across models (this is a common practice when comparing representational geometry across model families). \citep{Kipf2017, Pahng2024}

\paragraph{GAT internals (hidden / heads / concat).} We follow the original GAT recipe (hidden=8 per head, heads=8) and use \texttt{concat=True} for intermediate layers and \texttt{concat=False} for the final layer, consistent with the ICLR GAT paper (concatenation across heads in intermediate layers increases expressivity; final layer averages/does not concat to produce logits). This preserves the architectural inductive biases shown to improve performance on node-classification benchmarks. \citep{Velickovic2018}

\paragraph{SAE embedding size.} For the SAE used to probe GNN activations we set the SAE latent/hidden dimension to 64 to match the GNN second-layer output. Fixing SAE and GNN activation dimensionality avoids confounding motif-detection performance with representation capacity, and follows the standardization practice used in recent embedding-comparison studies. \citep{Kojaku2024, Pahng2024}

\paragraph{Masking probability (\texttt{mask\_prob}).} We use \texttt{mask\_prob = 0.2}. Masked graph autoencoder work has shown that reconstruction performance and downstream transfer vary with mask ratio and that conservative masking ratios (e.g., $0.1$--$0.3$) are often a stable starting point for discriminative downstream tasks, while larger ratios may be workable but can degrade performance depending on redundancy in node features and graph homophily \citep{Hou2022,MaskGAE2023}. We therefore choose 0.2 as a conservative masking rate that provides a meaningful reconstruction task while retaining sufficient observed context for stable SAE learning.

\paragraph{Optimizer, learning rate and training schedule.} We use Adam with a baseline learning rate \texttt{lr=1e-3}. A learning rate of $10^{-3}$ is a common default in modern GNN work and appears frequently in both empirical GNN studies and GNN AutoML search ranges; it provides a stable baseline for comparisons across architectures (we report experiments with identical optimizer settings for all models). Where datasets or tasks require it we perform grid searches over $\{1\mathrm{e}{-4},5\mathrm{e}{-4},1\mathrm{e}{-3},5\mathrm{e}{-3},1\mathrm{e}{-2}\}$ as suggested in GNN hyperparameter surveys. \citep{ZhouAutoML, Knyazev2019}

\paragraph{Epochs and early stopping.} We train for up to \texttt{num\_epochs=100} with early stopping (validation patience $=25$). A 100-epoch budget combined with a moderate patience value offers sufficient time for convergence on the medium-scale benchmarks used in this study while avoiding overfitting; patience of 20–50 epochs is common in applied deep learning and has been used in recent GNN / autoencoder papers as a conservative stopping heuristic \citep{Prechelt1998, GraphMAE22}. We also save the best validation checkpoint and report metrics from that checkpoint.

\paragraph{Concat vs.\ final projection.} The choice to \texttt{concat=True} in intermediate attention layers (and \texttt{concat=False} in the final layer) follows the original GAT design: concatenation across heads increases representational capacity mid-network, whereas the final layer projects to logits without concatenation for stable classification outputs \citep{Velickovic2018}. When comparing representations we keep this canonical behavior to ensure our GAT baseline mirrors the original design.

\paragraph{Practical notes and reproducibility.} For every reported experiment we (i) fix the random seeds, (ii) run each model across multiple seeds and report mean $\pm$ std, and (iii) provide a short config file in the supplementary material with the exact training script, optimizer state, and checkpointing procedure. Where hyperparameters were varied (learning rate, mask\_prob) we include a small grid search described in the Supplementary Materials.

@article{Misato2024,
  title={MISATO: Mechanistic Interpretability for Structured and Topological Operators},
  author={YourNameIfKnown, et al.},
  journal={arXiv preprint arXiv:240X.XXXXX},
  year={2024},
  note={An interpretability framework that analyzes GNN circuits using sparse autoencoders; recommends batch sizes 64--128 for stable feature extraction.}
}

@inproceedings{Kipf2017,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N. and Welling, Max},
  booktitle={ICLR},
  year={2017},
  url={https://arxiv.org/abs/1609.02907}
}

@inproceedings{Velickovic2018,
  title={Graph Attention Networks},
  author={Veli\v{c}kovi\'c, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  booktitle={ICLR},
  year={2018},
  url={https://arxiv.org/abs/1710.10903}
}

@article{Hou2022_GraphMAE,
  title={GraphMAE: Self-Supervised Masked Graph Autoencoders},
  author={Hou, Zhijie and others},
  journal={arXiv preprint},
  year={2022},
  url={https://arxiv.org/abs/2205.10803}
}

@inproceedings{MaskGAE2023,
  title={What's Behind the Mask: Understanding Masked Graph Autoencoders},
  author={Edison Leee and others},
  booktitle={KDD 2023 / related workshop},
  year={2023},
  url={https://dl.acm.org/doi/10.1145/3580305.3599546}
}

@article{Knyazev2019,
  title={Understanding Attention and Generalization in Graph Neural Networks},
  author={Knyazev, Boris and Lee, S and others},
  year={2019},
  url={https://papers.neurips.cc/paper/8673-understanding-attention-and-generalization-in-graph-neural-networks.pdf}
}

@misc{ZhouAutoML,
  title={AutoML — Graph Neural Networks (hyperparameter recommendations)},
  author={Zhou, Kai and others},
  year={2022},
  url={https://graph-neural-networks.github.io}
}

@misc{Prechelt1998,
  title={Early Stopping — But When?},
  author={Prechelt, Lutz},
  year={1998},
  note={classic early-stopping guidance widely cited in practice}
}

