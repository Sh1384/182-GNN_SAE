{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Latent Features vs. Graph Motifs Correspondence Analysis\n",
    "\n",
    "This notebook analyzes whether individual SAE latent features correspond to canonical graph motifs.\n",
    "\n",
    "**Approach:**\n",
    "- Use ONLY test set (500 graphs, ~40K nodes)\n",
    "- Extract 512-dimensional latent representations for each node using trained SAE\n",
    "- Analyze correlation between latent features (z1-z512) and motif membership labels\n",
    "- Compute point-biserial correlation, precision/recall, and mutual information\n",
    "- Identify interpretable (rpb > 0.5) and monosemantic (rpb > 0.7) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff04c8cb430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SAE Model and Test Graph IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded trained SAE model\n",
      "  Architecture: 64 -> 512 -> 64\n",
      "  Sparsity: TopK with k=32 (6.25% active)\n"
     ]
    }
   ],
   "source": [
    "# Define SAE architecture (must match training)\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    \"\"\"Sparse Autoencoder with TopK sparsity.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 64, latent_dim: int = 512, k: int = 32):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.k = k\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim, latent_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode with TopK sparsity.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        z = F.relu(z)\n",
    "        \n",
    "        if self.k < self.latent_dim:\n",
    "            topk_values, topk_indices = torch.topk(z, self.k, dim=1)\n",
    "            z_sparse = torch.zeros_like(z)\n",
    "            z_sparse.scatter_(1, topk_indices, topk_values)\n",
    "            return z_sparse\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n",
    "\n",
    "# Load trained SAE model\n",
    "model = SparseAutoencoder(input_dim=64, latent_dim=512, k=32)\n",
    "checkpoint = torch.load('checkpoints/sae_model.pt', weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Loaded trained SAE model\")\n",
    "print(f\"  Architecture: {model.input_dim} -> {model.latent_dim} -> {model.input_dim}\")\n",
    "print(f\"  Sparsity: TopK with k={model.k} ({100*model.k/model.latent_dim:.2f}% active)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded 500 test graph IDs\n",
      "  Range: 5 to 4994\n"
     ]
    }
   ],
   "source": [
    "# Load test graph IDs\n",
    "with open('outputs/test_graph_ids.json', 'r') as f:\n",
    "    test_graph_ids = json.load(f)['graph_ids']\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(test_graph_ids)} test graph IDs\")\n",
    "print(f\"  Range: {min(test_graph_ids)} to {max(test_graph_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract SAE Latent Representations for Test Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting SAE latent representations for test nodes...\n",
      "Reading from: outputs/activations/layer2/test\n",
      "Metadata from: virtual_graphs/data/all_graphs/graph_motif_metadata\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test graphs: 100%|██████████| 500/500 [00:01<00:00, 375.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Extracted latent representations for 5000 nodes\n",
      "  Latent features shape: (5000, 514)\n",
      "  Motif labels shape: (5000, 6)\n"
     ]
    }
   ],
   "source": [
    "# Extract latent representations for all test nodes\n",
    "activation_dir = Path(\"outputs/activations/layer2/test\")\n",
    "metadata_dir = Path(\"virtual_graphs/data/all_graphs/graph_motif_metadata\")\n",
    "\n",
    "all_latents = []  # Will store (graph_id, node_idx, z1, z2, ..., z512)\n",
    "all_motifs = []   # Will store motif labels for each node\n",
    "\n",
    "print(\"Extracting SAE latent representations for test nodes...\")\n",
    "print(f\"Reading from: {activation_dir}\")\n",
    "print(f\"Metadata from: {metadata_dir}\\n\")\n",
    "\n",
    "for graph_id in tqdm(test_graph_ids, desc=\"Processing test graphs\"):\n",
    "    # Load layer2 activations for this graph\n",
    "    act_file = activation_dir / f\"graph_{graph_id}.pt\"\n",
    "    if not act_file.exists():\n",
    "        print(f\"Warning: Missing activation file for graph {graph_id}\")\n",
    "        continue\n",
    "    \n",
    "    activations = torch.load(act_file, weights_only=True)  # [num_nodes, 64]\n",
    "    \n",
    "    # Extract latent representations using SAE encoder\n",
    "    with torch.no_grad():\n",
    "        latents = model.encode(activations)  # [num_nodes, 512]\n",
    "    \n",
    "    latents_np = latents.numpy()  # [num_nodes, 512]\n",
    "    num_nodes = latents_np.shape[0]\n",
    "    \n",
    "    # Load motif metadata for this graph\n",
    "    metadata_file = metadata_dir / f\"graph_{graph_id}_metadata.csv\"\n",
    "    if not metadata_file.exists():\n",
    "        print(f\"Warning: Missing metadata for graph {graph_id}\")\n",
    "        continue\n",
    "    \n",
    "    df_meta = pd.read_csv(metadata_file, index_col=0)\n",
    "    \n",
    "    # Verify node count matches\n",
    "    if len(df_meta) != num_nodes:\n",
    "        print(f\"Warning: Node count mismatch for graph {graph_id}: {num_nodes} vs {len(df_meta)}\")\n",
    "        continue\n",
    "    \n",
    "    # Store latents with graph_id and node_idx\n",
    "    for node_idx in range(num_nodes):\n",
    "        latent_row = [graph_id, node_idx] + latents_np[node_idx].tolist()\n",
    "        all_latents.append(latent_row)\n",
    "        \n",
    "        # Store motif labels for this node\n",
    "        motif_row = df_meta.iloc[node_idx].to_dict()\n",
    "        motif_row['graph_id'] = graph_id\n",
    "        motif_row['node_idx'] = node_idx\n",
    "        all_motifs.append(motif_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "latent_cols = ['graph_id', 'node_idx'] + [f'z{i+1}' for i in range(512)]\n",
    "df_latents = pd.DataFrame(all_latents, columns=latent_cols)\n",
    "\n",
    "df_motifs = pd.DataFrame(all_motifs)\n",
    "\n",
    "print(f\"\\n✓ Extracted latent representations for {len(df_latents)} nodes\")\n",
    "print(f\"  Latent features shape: {df_latents.shape}\")\n",
    "print(f\"  Motif labels shape: {df_motifs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Merged dataset: (5000, 518)\n",
      "\n",
      "Motif types found in metadata:\n",
      "  ['feedforward_loop', 'feedback_loop', 'single_input_module', 'cascade']\n",
      "\n",
      "Motif distribution in test set:\n",
      "  in_feedforward_loop: 1116 nodes (22.32%)\n",
      "  in_feedback_loop: 1164 nodes (23.28%)\n",
      "  in_single_input_module: 1109 nodes (22.18%)\n",
      "  in_cascade: 968 nodes (19.36%)\n",
      "\n",
      "Sample of merged dataset:\n",
      "   graph_id  node_idx   z1   z2   z3  z511  z512  in_feedforward_loop  \\\n",
      "0      4367         0  0.0  0.0  0.0   0.0   0.0                    1   \n",
      "1      4367         1  0.0  0.0  0.0   0.0   0.0                    1   \n",
      "2      4367         2  0.0  0.0  0.0   0.0   0.0                    1   \n",
      "3      4367         3  0.0  0.0  0.0   0.0   0.0                    0   \n",
      "4      4367         4  0.0  0.0  0.0   0.0   0.0                    0   \n",
      "\n",
      "   in_feedback_loop  in_single_input_module  in_cascade  \n",
      "0                 0                       0           0  \n",
      "1                 0                       0           0  \n",
      "2                 0                       0           0  \n",
      "3                 0                       0           1  \n",
      "4                 0                       0           1  \n"
     ]
    }
   ],
   "source": [
    "# Merge latents and motifs into single dataframe\n",
    "df = pd.merge(df_latents, df_motifs, on=['graph_id', 'node_idx'])\n",
    "\n",
    "print(f\"\\n✓ Merged dataset: {df.shape}\")\n",
    "print(f\"\\nMotif types found in metadata:\")\n",
    "motif_types = [col for col in df.columns if col.startswith('in_') or col in ['feedforward_loop', 'feedback_loop', 'single_input_module', 'cascade']]\n",
    "print(f\"  {motif_types}\")\n",
    "\n",
    "# Standardize column names if needed\n",
    "if 'feedforward_loop' in df.columns:\n",
    "    df = df.rename(columns={\n",
    "        'feedforward_loop': 'in_feedforward_loop',\n",
    "        'feedback_loop': 'in_feedback_loop',\n",
    "        'single_input_module': 'in_single_input_module',\n",
    "        'cascade': 'in_cascade'\n",
    "    })\n",
    "\n",
    "motif_types = ['in_feedforward_loop', 'in_feedback_loop', 'in_single_input_module', 'in_cascade']\n",
    "\n",
    "print(f\"\\nMotif distribution in test set:\")\n",
    "for motif in motif_types:\n",
    "    if motif in df.columns:\n",
    "        count = df[motif].sum()\n",
    "        pct = 100 * count / len(df)\n",
    "        print(f\"  {motif}: {count} nodes ({pct:.2f}%)\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nSample of merged dataset:\")\n",
    "display_cols = ['graph_id', 'node_idx', 'z1', 'z2', 'z3', 'z511', 'z512'] + motif_types\n",
    "print(df[display_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Point-Biserial Correlation Analysis\n",
    "\n",
    "Compute correlation between each SAE latent feature (z1-z512) and binary motif labels.\n",
    "\n",
    "$$r_{pb}(z_j, m) = \\frac{\\bar{z}_1 - \\bar{z}_0}{s_z} \\sqrt{\\frac{n_1 n_0}{n^2}}$$\n",
    "\n",
    "Where:\n",
    "- $\\bar{z}_1$: mean activation when motif is present\n",
    "- $\\bar{z}_0$: mean activation when motif is absent\n",
    "- $s_z$: standard deviation of feature\n",
    "- $n_1, n_0$: counts in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing point-biserial correlations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Motif types:   0%|          | 0/4 [00:00<?, ?it/s]/data/users/goodarzilab/shervin/miniconda3/lib/python3.12/site-packages/scipy/stats/_stats_py.py:5655: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "Motif types: 100%|██████████| 4/4 [00:02<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Computed 2048 correlations (512 features × 4 motifs)\n",
      "\n",
      "Correlation statistics:\n",
      "              rpb     rpb_abs          pval\n",
      "count  624.000000  624.000000  6.240000e+02\n",
      "mean    -0.002357    0.015667  4.147999e-01\n",
      "std      0.021080    0.014287  2.912442e-01\n",
      "min     -0.093109    0.000027  5.296589e-16\n",
      "25%     -0.012794    0.006930  1.305904e-01\n",
      "50%     -0.004175    0.011163  4.300287e-01\n",
      "75%      0.009564    0.021383  6.241967e-01\n",
      "max      0.114268    0.114268  9.984975e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute point-biserial correlation for all feature-motif pairs\n",
    "latent_features = [f'z{i+1}' for i in range(512)]\n",
    "\n",
    "correlations = []\n",
    "\n",
    "print(\"Computing point-biserial correlations...\")\n",
    "for motif in tqdm(motif_types, desc=\"Motif types\"):\n",
    "    if motif not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    for z_idx, z_col in enumerate(latent_features):\n",
    "        # Compute point-biserial correlation\n",
    "        corr, pval = pointbiserialr(df[motif], df[z_col])\n",
    "        \n",
    "        correlations.append({\n",
    "            'feature': z_col,\n",
    "            'feature_idx': z_idx + 1,\n",
    "            'motif': motif,\n",
    "            'rpb': corr,\n",
    "            'pval': pval,\n",
    "            'rpb_abs': abs(corr)\n",
    "        })\n",
    "\n",
    "df_corr = pd.DataFrame(correlations)\n",
    "\n",
    "print(f\"\\n✓ Computed {len(df_corr)} correlations ({len(latent_features)} features × {len(motif_types)} motifs)\")\n",
    "print(f\"\\nCorrelation statistics:\")\n",
    "print(df_corr[['rpb', 'rpb_abs', 'pval']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-Motif Correlation Matrix (rpb):\n",
      "motif    in_cascade  in_feedback_loop  in_feedforward_loop  \\\n",
      "feature                                                      \n",
      "z1        -0.006930          0.025676            -0.007581   \n",
      "z10             NaN               NaN                  NaN   \n",
      "z100            NaN               NaN                  NaN   \n",
      "z101            NaN               NaN                  NaN   \n",
      "z102            NaN               NaN                  NaN   \n",
      "z103       0.030967          0.031207            -0.013237   \n",
      "z104            NaN               NaN                  NaN   \n",
      "z105      -0.006930         -0.007791            -0.007581   \n",
      "z106      -0.004497         -0.014220             0.003696   \n",
      "z107            NaN               NaN                  NaN   \n",
      "\n",
      "motif    in_single_input_module  \n",
      "feature                          \n",
      "z1                    -0.007551  \n",
      "z10                         NaN  \n",
      "z100                        NaN  \n",
      "z101                        NaN  \n",
      "z102                        NaN  \n",
      "z103                  -0.050395  \n",
      "z104                        NaN  \n",
      "z105                  -0.007551  \n",
      "z106                  -0.027611  \n",
      "z107                        NaN  \n",
      "\n",
      "✓ Saved full correlation matrix to outputs/feature_motif_correlations.csv\n"
     ]
    }
   ],
   "source": [
    "# Pivot to create feature-motif correlation matrix\n",
    "corr_matrix = df_corr.pivot(index='feature', columns='motif', values='rpb')\n",
    "\n",
    "print(\"Feature-Motif Correlation Matrix (rpb):\")\n",
    "print(corr_matrix.head(10))\n",
    "\n",
    "# Save full correlation matrix\n",
    "corr_matrix.to_csv('outputs/feature_motif_correlations.csv')\n",
    "print(f\"\\n✓ Saved full correlation matrix to outputs/feature_motif_correlations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identify Interpretable and Monosemantic Features\n",
    "\n",
    "**Interpretable feature:** rpb > 0.5 for one motif, rpb < 0.2 for all others\n",
    "\n",
    "**Monosemantic feature:** rpb > 0.7 for one motif, rpb < 0.2 for all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identify interpretable and monosemantic features\nINTERPRETABLE_THRESHOLD = 0.5\nMONOSEMANTIC_THRESHOLD = 0.7\nOTHER_MOTIF_THRESHOLD = 0.2\n\ninterpretable_features = []\nmonosemantic_features = []\n\n# Count features with valid correlations\nvalid_features = 0\n\nfor feature in latent_features:\n    feature_corrs = corr_matrix.loc[feature].abs()\n    \n    # Skip features with all NaN correlations (never activated)\n    if feature_corrs.isna().all():\n        continue\n    \n    valid_features += 1\n    \n    # Get max correlation (ignoring NaN)\n    max_corr = feature_corrs.max(skipna=True)\n    max_motif = feature_corrs.idxmax(skipna=True)\n    \n    # Skip if max is still NaN\n    if pd.isna(max_corr) or pd.isna(max_motif):\n        continue\n    \n    # Check other motifs\n    other_corrs = feature_corrs.drop(max_motif)\n    max_other = other_corrs.max(skipna=True)\n    \n    # Handle case where other correlations might all be NaN\n    if pd.isna(max_other):\n        max_other = 0.0\n    \n    # Interpretable: max > 0.5 and others < 0.2\n    if max_corr > INTERPRETABLE_THRESHOLD and max_other < OTHER_MOTIF_THRESHOLD:\n        interpretable_features.append({\n            'feature': feature,\n            'motif': max_motif,\n            'rpb': corr_matrix.loc[feature, max_motif],\n            'rpb_abs': max_corr,\n            'max_other_rpb': max_other\n        })\n        \n        # Monosemantic: max > 0.7 and others < 0.2\n        if max_corr > MONOSEMANTIC_THRESHOLD:\n            monosemantic_features.append({\n                'feature': feature,\n                'motif': max_motif,\n                'rpb': corr_matrix.loc[feature, max_motif],\n                'rpb_abs': max_corr,\n                'max_other_rpb': max_other\n            })\n\ndf_interpretable = pd.DataFrame(interpretable_features)\ndf_monosemantic = pd.DataFrame(monosemantic_features)\n\nprint(\"=\" * 60)\nprint(\"INTERPRETABILITY ANALYSIS\")\nprint(\"=\" * 60)\nprint(f\"\\nTotal features: {len(latent_features)}\")\nprint(f\"Features with valid correlations: {valid_features} ({100*valid_features/len(latent_features):.1f}%)\")\nprint(f\"Features never activated (all NaN): {len(latent_features) - valid_features}\")\nprint(f\"\\nInterpretable features (rpb > {INTERPRETABLE_THRESHOLD}, others < {OTHER_MOTIF_THRESHOLD}): {len(df_interpretable)}\")\nprint(f\"Monosemantic features (rpb > {MONOSEMANTIC_THRESHOLD}, others < {OTHER_MOTIF_THRESHOLD}): {len(df_monosemantic)}\")\n\nif len(df_interpretable) > 0:\n    print(f\"\\nInterpretable features by motif:\")\n    print(df_interpretable.groupby('motif').size())\n\nif len(df_monosemantic) > 0:\n    print(f\"\\nMonosemantic features by motif:\")\n    print(df_monosemantic.groupby('motif').size())\n    \n    print(f\"\\nTop monosemantic features:\")\n    print(df_monosemantic.sort_values('rpb_abs', ascending=False).head(10))\nelse:\n    print(f\"\\nNo monosemantic features found with current thresholds.\")\n    print(f\"This suggests the SAE features may not have strong one-to-one correspondence with motifs.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Precision and Recall Analysis\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\#\\text{ nodes where } (z_j > \\tau) \\text{ and motif present}}{\\#\\text{ nodes where } (z_j > \\tau)}$$\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\#\\text{ nodes where } (z_j > \\tau) \\text{ and motif present}}{\\#\\text{ nodes with motif present}}$$\n",
    "\n",
    "where $\\tau$ is the 95th percentile activation threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_recall(df, feature, motif, percentile=95):\n",
    "    \"\"\"Compute precision and recall for a feature-motif pair.\"\"\"\n",
    "    # Threshold at specified percentile\n",
    "    threshold = np.percentile(df[feature], percentile)\n",
    "    \n",
    "    # Nodes where feature is highly activated\n",
    "    activated = df[feature] > threshold\n",
    "    \n",
    "    # Nodes where motif is present\n",
    "    present = df[motif] == 1\n",
    "    \n",
    "    # True positives\n",
    "    tp = (activated & present).sum()\n",
    "    \n",
    "    # Precision and recall\n",
    "    precision = tp / activated.sum() if activated.sum() > 0 else 0\n",
    "    recall = tp / present.sum() if present.sum() > 0 else 0\n",
    "    \n",
    "    return precision, recall, threshold\n",
    "\n",
    "# Compute precision/recall for top correlated features\n",
    "precision_recall_results = []\n",
    "\n",
    "print(\"Computing precision and recall for top features...\")\n",
    "\n",
    "# For each motif, get top 10 features by absolute correlation\n",
    "for motif in motif_types:\n",
    "    if motif not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    motif_corrs = df_corr[df_corr['motif'] == motif].nlargest(10, 'rpb_abs')\n",
    "    \n",
    "    for _, row in motif_corrs.iterrows():\n",
    "        feature = row['feature']\n",
    "        precision, recall, threshold = compute_precision_recall(df, feature, motif)\n",
    "        \n",
    "        precision_recall_results.append({\n",
    "            'feature': feature,\n",
    "            'motif': motif,\n",
    "            'rpb': row['rpb'],\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'threshold': threshold,\n",
    "            'f1_score': 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        })\n",
    "\n",
    "df_pr = pd.DataFrame(precision_recall_results)\n",
    "\n",
    "print(f\"\\n✓ Computed precision/recall for {len(df_pr)} feature-motif pairs\")\n",
    "print(f\"\\nPrecision/Recall statistics:\")\n",
    "print(df_pr[['precision', 'recall', 'f1_score']].describe())\n",
    "\n",
    "print(f\"\\nTop 10 by F1 score:\")\n",
    "print(df_pr.nlargest(10, 'f1_score')[['feature', 'motif', 'rpb', 'precision', 'recall', 'f1_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mutual Information Analysis\n",
    "\n",
    "$$I(z_j; m) = H(m) - H(m|z_j)$$\n",
    "\n",
    "Captures nonlinear dependencies between features and motifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mutual information\n",
    "mi_results = []\n",
    "\n",
    "print(\"Computing mutual information...\")\n",
    "\n",
    "for motif in tqdm(motif_types, desc=\"Motif types\"):\n",
    "    if motif not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    for z_col in latent_features:\n",
    "        # Discretize continuous latent feature into bins\n",
    "        z_discrete = pd.cut(df[z_col], bins=10, labels=False)\n",
    "        \n",
    "        # Compute mutual information\n",
    "        mi = mutual_info_score(df[motif], z_discrete)\n",
    "        \n",
    "        mi_results.append({\n",
    "            'feature': z_col,\n",
    "            'motif': motif,\n",
    "            'mutual_info': mi\n",
    "        })\n",
    "\n",
    "df_mi = pd.DataFrame(mi_results)\n",
    "\n",
    "print(f\"\\n✓ Computed {len(df_mi)} mutual information scores\")\n",
    "print(f\"\\nMutual Information statistics:\")\n",
    "print(df_mi['mutual_info'].describe())\n",
    "\n",
    "# Pivot to matrix form\n",
    "mi_matrix = df_mi.pivot(index='feature', columns='motif', values='mutual_info')\n",
    "mi_matrix.to_csv('outputs/feature_motif_mutual_info.csv')\n",
    "print(f\"\\n✓ Saved MI matrix to outputs/feature_motif_mutual_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Correlation heatmap for top features\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 12))\n",
    "\n",
    "# Get top 50 features by max absolute correlation\n",
    "max_corrs = corr_matrix.abs().max(axis=1)\n",
    "top_features = max_corrs.nlargest(50).index\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(corr_matrix.loc[top_features], \n",
    "            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            cbar_kws={'label': 'Point-Biserial Correlation (rpb)'},\n",
    "            ax=ax)\n",
    "ax.set_title('Top 50 SAE Features vs. Motifs (Point-Biserial Correlation)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Motif Type', fontsize=12)\n",
    "ax.set_ylabel('SAE Latent Feature', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/feature_motif_correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Distribution of correlation strengths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Histogram of absolute correlations\n",
    "ax = axes[0]\n",
    "for motif in motif_types:\n",
    "    if motif in df_corr['motif'].values:\n",
    "        motif_corrs = df_corr[df_corr['motif'] == motif]['rpb_abs']\n",
    "        ax.hist(motif_corrs, bins=50, alpha=0.5, label=motif.replace('in_', ''))\n",
    "\n",
    "ax.axvline(x=INTERPRETABLE_THRESHOLD, color='orange', linestyle='--', \n",
    "           linewidth=2, label=f'Interpretable (>{INTERPRETABLE_THRESHOLD})')\n",
    "ax.axvline(x=MONOSEMANTIC_THRESHOLD, color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Monosemantic (>{MONOSEMANTIC_THRESHOLD})')\n",
    "ax.set_xlabel('Absolute Correlation |rpb|', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Distribution of Feature-Motif Correlations', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Precision vs Recall scatter\n",
    "ax = axes[1]\n",
    "for motif in motif_types:\n",
    "    motif_pr = df_pr[df_pr['motif'] == motif]\n",
    "    ax.scatter(motif_pr['recall'], motif_pr['precision'], \n",
    "              alpha=0.6, s=100, label=motif.replace('in_', ''))\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='F1=0.5')\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision vs. Recall for Top Features', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/correlation_distribution_and_precision_recall.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Correlation vs Mutual Information scatter\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Merge correlation and MI data\n",
    "df_combined = pd.merge(df_corr, df_mi, on=['feature', 'motif'])\n",
    "\n",
    "for motif in motif_types:\n",
    "    motif_data = df_combined[df_combined['motif'] == motif]\n",
    "    ax.scatter(motif_data['rpb_abs'], motif_data['mutual_info'], \n",
    "              alpha=0.4, s=20, label=motif.replace('in_', ''))\n",
    "\n",
    "ax.set_xlabel('Absolute Correlation |rpb|', fontsize=12)\n",
    "ax.set_ylabel('Mutual Information', fontsize=12)\n",
    "ax.set_title('Point-Biserial Correlation vs. Mutual Information', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/correlation_vs_mutual_info.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Feature activation distributions for monosemantic features\n",
    "if len(df_monosemantic) > 0:\n",
    "    # Plot top 4 monosemantic features\n",
    "    top_mono = df_monosemantic.nlargest(4, 'rpb_abs')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_mono.iterrows()):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        feature = row['feature']\n",
    "        motif = row['motif']\n",
    "        rpb = row['rpb']\n",
    "        \n",
    "        # Plot activation distribution split by motif presence\n",
    "        motif_present = df[df[motif] == 1][feature]\n",
    "        motif_absent = df[df[motif] == 0][feature]\n",
    "        \n",
    "        ax.hist(motif_absent, bins=50, alpha=0.5, label='Motif Absent', color='gray')\n",
    "        ax.hist(motif_present, bins=50, alpha=0.7, label='Motif Present', color='red')\n",
    "        \n",
    "        ax.set_xlabel(f'{feature} Activation', fontsize=11)\n",
    "        ax.set_ylabel('Count', fontsize=11)\n",
    "        ax.set_title(f'{feature} for {motif.replace(\"in_\", \"\")}\\nrpb = {rpb:.3f}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/monosemantic_feature_activations.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No monosemantic features found to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Additional Visualizations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Plot 5: Feature Activation Frequency\n# Show which features are most frequently activated across the dataset\nfeature_activation_freq = []\nfor z_col in latent_features:\n    activated_count = (df[z_col] > 0).sum()\n    freq = activated_count / len(df)\n    feature_activation_freq.append({\n        'feature': z_col,\n        'activation_count': activated_count,\n        'activation_freq': freq\n    })\n\ndf_activation = pd.DataFrame(feature_activation_freq)\ndf_activation = df_activation.sort_values('activation_freq', ascending=False)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Left: Histogram of activation frequencies\nax = axes[0]\nax.hist(df_activation['activation_freq'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\nax.axvline(x=model.k/model.latent_dim, color='red', linestyle='--', \n           linewidth=2, label=f'Expected (k/n={model.k}/{model.latent_dim}={100*model.k/model.latent_dim:.1f}%)')\nax.set_xlabel('Activation Frequency (% of nodes)', fontsize=12)\nax.set_ylabel('Number of Features', fontsize=12)\nax.set_title('Distribution of Feature Activation Frequencies', fontsize=14, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\n\n# Right: Top 30 most frequently activated features\nax = axes[1]\ntop_30 = df_activation.head(30)\nax.barh(range(len(top_30)), top_30['activation_freq'], color='steelblue', alpha=0.7)\nax.set_yticks(range(len(top_30)))\nax.set_yticklabels(top_30['feature'])\nax.set_xlabel('Activation Frequency', fontsize=12)\nax.set_ylabel('Feature', fontsize=12)\nax.set_title('Top 30 Most Frequently Activated Features', fontsize=14, fontweight='bold')\nax.invert_yaxis()\nax.grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.savefig('outputs/feature_activation_frequency.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nFeature Activation Statistics:\")\nprint(f\"  Mean activation freq: {df_activation['activation_freq'].mean():.3f}\")\nprint(f\"  Median activation freq: {df_activation['activation_freq'].median():.3f}\")\nprint(f\"  Features never activated: {(df_activation['activation_freq'] == 0).sum()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Plot 6: Top features for each motif type\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.ravel()\n\nfor idx, motif in enumerate(motif_types):\n    ax = axes[idx]\n    \n    # Get top 15 features for this motif\n    motif_corrs = df_corr[df_corr['motif'] == motif].nlargest(15, 'rpb_abs')\n    \n    # Create bar plot\n    colors = ['red' if rpb > 0 else 'blue' for rpb in motif_corrs['rpb']]\n    ax.barh(range(len(motif_corrs)), motif_corrs['rpb'], color=colors, alpha=0.7)\n    ax.set_yticks(range(len(motif_corrs)))\n    ax.set_yticklabels(motif_corrs['feature'])\n    ax.set_xlabel('Point-Biserial Correlation (rpb)', fontsize=11)\n    ax.set_ylabel('SAE Feature', fontsize=11)\n    ax.set_title(f'Top 15 Features for {motif.replace(\"in_\", \"\").title()}', \n                 fontsize=12, fontweight='bold')\n    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n    ax.axvline(x=INTERPRETABLE_THRESHOLD, color='orange', linestyle='--', \n               linewidth=1.5, alpha=0.5, label='Interpretable')\n    ax.axvline(x=-INTERPRETABLE_THRESHOLD, color='orange', linestyle='--', \n               linewidth=1.5, alpha=0.5)\n    ax.invert_yaxis()\n    ax.grid(True, alpha=0.3, axis='x')\n    ax.legend(fontsize=9)\n\nplt.tight_layout()\nplt.savefig('outputs/top_features_per_motif.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Tables and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive summary table\nsummary_results = []\n\n# For each feature, get its top motif correlation and related metrics\nfor feature in latent_features:\n    feature_corrs = corr_matrix.loc[feature].abs()\n    \n    # Skip features with all NaN correlations\n    if feature_corrs.isna().all():\n        continue\n    \n    max_corr = feature_corrs.max(skipna=True)\n    max_motif = feature_corrs.idxmax(skipna=True)\n    \n    # Skip if max is still NaN\n    if pd.isna(max_corr) or pd.isna(max_motif):\n        continue\n    \n    # Get correlation value (with sign)\n    corr_value = corr_matrix.loc[feature, max_motif]\n    \n    # Get MI for this feature-motif pair\n    mi_value = mi_matrix.loc[feature, max_motif]\n    \n    # Get precision/recall if available\n    pr_row = df_pr[(df_pr['feature'] == feature) & (df_pr['motif'] == max_motif)]\n    if len(pr_row) > 0:\n        precision = pr_row.iloc[0]['precision']\n        recall = pr_row.iloc[0]['recall']\n        f1 = pr_row.iloc[0]['f1_score']\n    else:\n        precision, recall, _ = compute_precision_recall(df, feature, max_motif)\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Check other motifs\n    other_corrs = feature_corrs.drop(max_motif)\n    max_other = other_corrs.max(skipna=True)\n    if pd.isna(max_other):\n        max_other = 0.0\n    \n    # Classify\n    is_interpretable = (max_corr > INTERPRETABLE_THRESHOLD) and (max_other < OTHER_MOTIF_THRESHOLD)\n    is_monosemantic = (max_corr > MONOSEMANTIC_THRESHOLD) and (max_other < OTHER_MOTIF_THRESHOLD)\n    \n    summary_results.append({\n        'feature': feature,\n        'top_motif': max_motif,\n        'rpb': corr_value,\n        'rpb_abs': max_corr,\n        'mutual_info': mi_value,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'max_other_rpb': max_other,\n        'interpretable': is_interpretable,\n        'monosemantic': is_monosemantic\n    })\n\ndf_summary = pd.DataFrame(summary_results)\n\n# Sort by absolute correlation\ndf_summary = df_summary.sort_values('rpb_abs', ascending=False)\n\n# Save summary\ndf_summary.to_csv('outputs/feature_motif_summary.csv', index=False)\nprint(f\"✓ Saved comprehensive summary to outputs/feature_motif_summary.csv\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"FINAL SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"\\nTotal nodes analyzed: {len(df):,}\")\nprint(f\"Total features: {len(latent_features)}\")\nprint(f\"Features with valid data: {len(df_summary)}\")\nprint(f\"Interpretable features: {df_summary['interpretable'].sum()}\")\nprint(f\"Monosemantic features: {df_summary['monosemantic'].sum()}\")\n\nprint(f\"\\nTop 20 features by correlation:\")\nprint(df_summary.head(20)[['feature', 'top_motif', 'rpb', 'mutual_info', 'precision', 'recall', 'interpretable', 'monosemantic']])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print interpretable/monosemantic features grouped by motif\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INTERPRETABLE FEATURES BY MOTIF\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for motif in motif_types:\n",
    "    motif_features = df_summary[(df_summary['top_motif'] == motif) & (df_summary['interpretable'] == True)]\n",
    "    mono_features = df_summary[(df_summary['top_motif'] == motif) & (df_summary['monosemantic'] == True)]\n",
    "    \n",
    "    print(f\"\\n{motif.replace('in_', '').upper()}:\")\n",
    "    print(f\"  Interpretable: {len(motif_features)}\")\n",
    "    print(f\"  Monosemantic: {len(mono_features)}\")\n",
    "    \n",
    "    if len(mono_features) > 0:\n",
    "        print(f\"  Top monosemantic features:\")\n",
    "        for _, row in mono_features.nlargest(5, 'rpb_abs').iterrows():\n",
    "            print(f\"    {row['feature']}: rpb={row['rpb']:.3f}, MI={row['mutual_info']:.4f}, P={row['precision']:.3f}, R={row['recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "This analysis evaluated whether SAE latent features correspond to canonical graph motifs using:\n",
    "\n",
    "1. **Point-biserial correlation** - Linear relationship between continuous features and binary motif labels\n",
    "2. **Precision/Recall** - Specificity and coverage of features for motifs\n",
    "3. **Mutual Information** - Nonlinear dependencies as validation\n",
    "\n",
    "Features were classified as:\n",
    "- **Interpretable**: rpb > 0.5 for one motif, < 0.2 for others\n",
    "- **Monosemantic**: rpb > 0.7 for one motif, < 0.2 for others (one-to-one correspondence)\n",
    "\n",
    "Results show whether the SAE successfully learned to decompose GNN representations into interpretable motif-specific features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}